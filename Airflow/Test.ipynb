{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged: amazon_products_20241211_120944.csv\n",
      "Merged: amazon_products_20241211_135603.csv\n",
      "Merged: amazon_products_20241211_145330.csv\n",
      "Merged: amazon_products_20241211_172619.csv\n",
      "Merged: amazon_products_20241211_204940.csv\n",
      "\n",
      "Merging complete!\n",
      "Total files merged: 5\n",
      "Output file: amazon_products1.csv\n",
      "Total rows in merged file: 7540\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def merge_csv_files(input_directory, output_file='merged_output.csv'):\n",
    "    # Ensure the input directory exists\n",
    "    if not os.path.exists(input_directory):\n",
    "        print(f\"Error: Directory {input_directory} does not exist.\")\n",
    "        return 0\n",
    "    \n",
    "    # List to store individual DataFrames\n",
    "    dataframes = []\n",
    "    \n",
    "    # Counter for merged files\n",
    "    merged_files_count = 0\n",
    "    \n",
    "    # Iterate through all files in the directory\n",
    "    for filename in os.listdir(input_directory):\n",
    "        # Check if file is a CSV\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(input_directory, filename)\n",
    "            try:\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Add a column to track the source file if desired\n",
    "                df['source_file'] = filename\n",
    "                \n",
    "                # Append to list of DataFrames\n",
    "                dataframes.append(df)\n",
    "                \n",
    "                # Increment counter\n",
    "                merged_files_count += 1\n",
    "                print(f\"Merged: {filename}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "    \n",
    "    # Check if any files were found\n",
    "    if not dataframes:\n",
    "        print(\"No CSV files found in the directory.\")\n",
    "        return 0\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_directory = os.path.dirname(output_file) or '.'\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    # Save merged DataFrame\n",
    "    merged_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"\\nMerging complete!\")\n",
    "    print(f\"Total files merged: {merged_files_count}\")\n",
    "    print(f\"Output file: {output_file}\")\n",
    "    print(f\"Total rows in merged file: {len(merged_df)}\")\n",
    "    \n",
    "    return merged_files_count\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    input_directory = 'D:/app/Bigdata-IS405.P11/Crawl/Airflow/amazon_scraper_output'  # Directory containing CSV files\n",
    "    output_file = 'amazon_products1.csv'  # Output merged file path1\n",
    "    \n",
    "    merge_csv_files(input_directory, output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract Product Title\n",
    "def get_title(soup):\n",
    "    try:\n",
    "        title = soup.find(\"span\", class_=\"a-size-base-plus a-color-base a-text-normal\").get_text(strip=True)\n",
    "    except AttributeError:\n",
    "        title = \"\"\n",
    "    return title\n",
    "\n",
    "# Function to extract Product Price\n",
    "def get_price(soup):\n",
    "    try:\n",
    "        price = soup.find('span', class_='a-price').find('span', class_='a-offscreen').text.strip()\n",
    "    except AttributeError:\n",
    "        price = \"\"\n",
    "    return price\n",
    "\n",
    "def get_old_price(soup):\n",
    "    try:\n",
    "        old_price = soup.find(\"div\", class_=\"a-section aok-inline-block\") \\\n",
    "                        .find(\"span\", class_=\"a-offscreen\").text.strip()\n",
    "    except AttributeError:\n",
    "        old_price = \"\"\n",
    "    return old_price\n",
    "# Function to extract Discount Percent\n",
    "def get_discount_percent(soup):\n",
    "    try:\n",
    "        price_whole = soup.find('span', class_='a-price-whole').text.strip()\n",
    "        price_decimal = soup.find('span', class_='a-price-decimal').next_sibling.strip()\n",
    "\n",
    "        discount_percent = price_whole + '.' + price_decimal\n",
    "    except AttributeError:\n",
    "        discount_percent = \"\"\n",
    "    return discount_percent\n",
    "\n",
    "# Function to extract Brand Name\n",
    "# def get_brand(soup):\n",
    "#     try:\n",
    "#         # Find the div with the specific class attributes\n",
    "#         brand_div = soup.find(\"div\", class_=\"width_common txt_color_1 space_bottom_3\")\n",
    "#         # Extract the brand name within the strong tag inside this div\n",
    "#         brand = brand_div.find(\"strong\").get_text(strip=True)\n",
    "#     except AttributeError:\n",
    "#         brand = \"\"\n",
    "#     return brand\n",
    "\n",
    "\n",
    "def get_product_url(soup):\n",
    "    try:\n",
    "        product_url = soup.find(\"a\", class_=\"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\")['href']\n",
    "        product_url = \"https://www.amazon.com\" + product_url\n",
    "    except (TypeError, AttributeError):\n",
    "        product_url = \"\"\n",
    "    return product_url\n",
    "\n",
    "\n",
    "# Function to extract Product Rating\n",
    "def get_rating(soup):\n",
    "    try:\n",
    "        rating = soup.find('i', class_='a-icon a-icon-star-small a-star-small-4-5').find('span', class_='a-icon-alt').text.strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        rating = \"\"\n",
    "    return rating\n",
    "\n",
    "# Function to extract Number of Reviews\n",
    "\n",
    "def get_review_count(soup):\n",
    "    try:\n",
    "        review_count = soup.find('span', class_='a-size-base s-underline-text').text.strip()\n",
    "    except AttributeError:\n",
    "        review_count = \"\"\n",
    "    return review_count\n",
    "\n",
    "\n",
    "# Function to extract Number of Purchases\n",
    "def get_purchase_count(soup):\n",
    "    try:\n",
    "        purchase_count = soup.find(\"span\", class_=\"a-size-base a-color-secondary\").get_text(strip=True)\n",
    "    except AttributeError:\n",
    "        purchase_count = \"\"\n",
    "    return purchase_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_details(product_url):\n",
    "    try:\n",
    "        HEADERS = ({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 OPR/112.0.0.0',\n",
    "            'Accept-Language': 'en-US, en;q=0.5'\n",
    "        })\n",
    "        page = requests.get(product_url, headers=HEADERS)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "        # Tìm bảng trong thẻ div chứa các thông tin chi tiết\n",
    "        details = {}\n",
    "        info_section = soup.find(\"div\", class_=\"a-section a-spacing-small a-spacing-top-small\")\n",
    "        \n",
    "        if info_section:\n",
    "            table = info_section.find(\"table\")\n",
    "            if table:\n",
    "                # Duyệt qua các hàng trong bảng và lấy thông tin\n",
    "                for row in table.find_all(\"tr\"):\n",
    "                    try:\n",
    "                        key = row.find(\"td\", class_=\"a-span3\").get_text(strip=True)\n",
    "                        value = row.find(\"td\", class_=\"a-span9\").get_text(strip=True)\n",
    "                        details[key] = value\n",
    "                    except AttributeError:\n",
    "                        continue\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        details = {}\n",
    "\n",
    "    return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_amazon(base_url, max_pages):\n",
    "    HEADERS = ({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36 OPR/114.0.0.0',\n",
    "        'Accept-Language': 'en-US, en;q=0.5'\n",
    "    })\n",
    "\n",
    "    d = {\n",
    "        \"title\": [], \"price\": [], \"old_price\": [], \n",
    "        \"product_url\": [], \"rating\": [], \"reviews\": [], \"purchases\": []\n",
    "    }\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{base_url}&page={page}\"\n",
    "        \n",
    "        print(f\"Scraping page {page} with URL: {url}\")\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Tìm kiếm sản phẩm trên trang\n",
    "        products = soup.find_all(\"div\", attrs={\"data-asin\": True})\n",
    "        if not products:\n",
    "            print(\"No products found on this page, stopping.\")\n",
    "            break  # Dừng nếu không có sản phẩm\n",
    "\n",
    "        for product in products:\n",
    "            d['title'].append(get_title(product))\n",
    "            d['price'].append(get_price(product))\n",
    "            d['old_price'].append(get_old_price(product))\n",
    "            product_url = get_product_url(product)\n",
    "            d['product_url'].append(product_url)\n",
    "            d['rating'].append(get_rating(product))\n",
    "            d['reviews'].append(get_review_count(product))\n",
    "            d['purchases'].append(get_purchase_count(product))\n",
    "\n",
    "            # Kiểm tra nếu product_url không rỗng trước khi gọi hàm get_product_details\n",
    "            if product_url:\n",
    "                details = get_product_details(product_url)\n",
    "                for key, value in details.items():\n",
    "                    # Thêm cột động vào từ điển `d` nếu chưa có\n",
    "                    if key not in d:\n",
    "                        d[key] = [\"\"] * len(d['title'])\n",
    "                    d[key].append(value)\n",
    "                \n",
    "                # Đảm bảo mỗi mục đã thêm đều có đúng số lượng phần tử\n",
    "                for key in d:\n",
    "                    if len(d[key]) < len(d['title']):\n",
    "                        d[key].append(\"\")\n",
    "            else:\n",
    "                for key in d:\n",
    "                    if len(d[key]) == len(d['title']) - 1:\n",
    "                        d[key].append(\"\")\n",
    "\n",
    "        # Thêm độ trễ để giảm khả năng bị chặn\n",
    "        time.sleep(5)\n",
    "\n",
    "    # Chuyển dữ liệu thành DataFrame và lưu vào CSV\n",
    "    amazon_df = pd.DataFrame.from_dict(d)\n",
    "    amazon_df['title'] = amazon_df['title'].replace('', np.nan)\n",
    "    amazon_df = amazon_df.dropna(subset=['title'])\n",
    "    amazon_df.to_csv(\"amazon_data.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 with URL: https://www.amazon.com/s?i=computers-intl-ship&bbn=16225007011&rh=n%3A16225007011%2Cn%3A1292110011&page=1\n",
      "Scraping page 2 with URL: https://www.amazon.com/s?i=computers-intl-ship&bbn=16225007011&rh=n%3A16225007011%2Cn%3A1292110011&page=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trieu\\AppData\\Local\\Temp\\ipykernel_10016\\3051676871.py:58: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  amazon_df['title'] = amazon_df['title'].replace('', np.nan)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    base_url = \"https://www.amazon.com/s?i=computers-intl-ship&bbn=16225007011&rh=n%3A16225007011%2Cn%3A1292110011\"  # Cập nhật link cơ sở\n",
    "    max_pages = 2  \n",
    "    scrape_amazon(base_url, max_pages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
